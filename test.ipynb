{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gymnasium: 1.2.0\n",
      "NumPy: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "print(\"Gymnasium:\", gym.__version__)\n",
    "print(\"NumPy:\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======= Iteración 1 =======\n",
      "[Evaluación de política] Iteraciones internas: 52\n",
      "Value Function--------------------------\n",
      "array([[ 0.6317923 , -6.58795902, -6.80453753, -6.61860071],\n",
      "       [-0.75683002, -4.85525532, -5.57063098, -5.11612663],\n",
      "       [-2.47364498, -4.21915994, -3.53812091, -0.98866673],\n",
      "       [-2.68471906, -3.44044543, -0.50218183,  3.02651158]])\n",
      "\n",
      "\n",
      "Policy--------------------------\n",
      "[['', '↓', '↓', '↓'],\n",
      " ['↑', '←', '↓', '↓'],\n",
      " ['←', '←', '↓', '↓'],\n",
      " ['←', '→', '→', '']]\n",
      "\n",
      "\n",
      "\n",
      "======= Iteración 2 =======\n",
      "[Evaluación de política] Iteraciones internas: 104\n",
      "Value Function--------------------------\n",
      "array([[49.99913966, 38.59930313, 33.7391814 , 38.5991814 ],\n",
      "       [49.9992257 , 43.99930313, 38.5991814 , 43.9991814 ],\n",
      "       [ 0.        , -1.        , 43.9991814 , 49.9991814 ],\n",
      "       [ 0.        , 43.9991814 , 49.9991814 , 49.9991814 ]])\n",
      "\n",
      "\n",
      "Policy--------------------------\n",
      "[['', '↓', '→', '↓'],\n",
      " ['↑', '←', '←', '↓'],\n",
      " ['↑', '↑', '→', '↓'],\n",
      " ['→', '→', '→', '']]\n",
      "\n",
      "\n",
      "\n",
      "======= Iteración 3 =======\n",
      "[Evaluación de política] Iteraciones internas: 3\n",
      "Value Function--------------------------\n",
      "array([[49.99937281, 38.59949198, 33.73947301, 38.59940324],\n",
      "       [49.99943553, 43.99949198, 38.59954278, 43.99940324],\n",
      "       [43.99949198, 38.59954278, 43.99940324, 49.99940324],\n",
      "       [38.59940324, 43.99940324, 49.99940324, 49.99940324]])\n",
      "\n",
      "\n",
      "Policy--------------------------\n",
      "[['', '↓', '↓', '↓'],\n",
      " ['↑', '←', '←', '↓'],\n",
      " ['↑', '↑', '→', '↓'],\n",
      " ['↑', '→', '→', '']]\n",
      "\n",
      "\n",
      "\n",
      "======= Iteración 4 =======\n",
      "[Evaluación de política] Iteraciones internas: 2\n",
      "Value Function--------------------------\n",
      "array([[49.99949198, 38.5995885 , 33.73962965, 38.59951662],\n",
      "       [49.99954278, 43.9995885 , 38.59962965, 43.99951662],\n",
      "       [43.9995885 , 38.59962965, 43.99951662, 49.99951662],\n",
      "       [38.59962965, 43.99951662, 49.99951662, 49.99951662]])\n",
      "\n",
      "\n",
      "Policy--------------------------\n",
      "[['', '↓', '↓', '↓'],\n",
      " ['↑', '←', '←', '↓'],\n",
      " ['↑', '↑', '→', '↓'],\n",
      " ['↑', '→', '→', '']]\n",
      "\n",
      "\n",
      "✅ Política estable encontrada. Algoritmo finalizado.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gridworld import GridWorldEnv\n",
    "from agent import PolicyIterationAgent\n",
    "\n",
    "env = GridWorldEnv()\n",
    "env.reset()\n",
    "\n",
    "## 0.9 is the agent discount factor\n",
    "pi_agent = PolicyIterationAgent(0.9)\n",
    "pi_agent.policyIterate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informe: Implementación de Policy Iteration en GridWorld\n",
    "\n",
    "## 1. Descripción general de la implementación\n",
    "El agente se implementó utilizando el algoritmo **Policy Iteration**, compuesto por dos fases principales: **policyIterate** y **policyImprove**.\n",
    "\n",
    "- **policyIterate**  \n",
    "  - Inicializa una política aleatoria para cada estado no terminal.  \n",
    "  - Evalúa la política actual calculando la función de valor \\( V(s) \\) de cada estado mediante iteraciones sucesivas hasta que la diferencia entre iteraciones sea menor que un umbral (\\(\\theta\\)), indicando convergencia.  \n",
    "  - Llama repetidamente a **policyImprove** para actualizar la política y verificar si es estable.  \n",
    "\n",
    "- **policyImprove**  \n",
    "  - Para cada estado, calcula el valor esperado de ejecutar cada acción posible considerando las transiciones y recompensas definidas por el entorno.  \n",
    "  - Selecciona la acción que maximiza dicho valor esperado.  \n",
    "  - Si en algún estado la acción óptima difiere de la acción actual de la política, marca la política como **no estable** para continuar iterando.  \n",
    "\n",
    "Ambas funciones trabajan de forma iterativa hasta que la política converge, es decir, no se producen cambios en ninguna acción para todos los estados.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Resultados obtenidos\n",
    "Durante la ejecución, el agente comenzó con una política aleatoria y, a través de cuatro iteraciones principales, alcanzó una política estable.  \n",
    "En cada iteración, se registraron:\n",
    "\n",
    "- **Número de iteraciones internas de evaluación**:  \n",
    "  - Iteración 1: 52  \n",
    "  - Iteración 2: 104  \n",
    "  - Iteración 3: 3  \n",
    "  - Iteración 4: 2  \n",
    "\n",
    "- **Evolución de la función de valor \\(V(s)\\)**:  \n",
    "  - Inicialmente, los valores eran bajos y negativos en varios estados, reflejando recompensas acumuladas pobres.  \n",
    "  - Posteriormente, los valores convergieron a cifras cercanas a **50** en estados favorables, lo que indica la maximización de recompensas a largo plazo.  \n",
    "\n",
    "- **Evolución de la política**:  \n",
    "  - Comenzó con direcciones dispersas y sin una estructura clara.  \n",
    "  - En la iteración final, la política muestra un camino consistente hacia los estados objetivo, evidenciando aprendizaje estable.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Análisis de convergencia\n",
    "El algoritmo mostró un patrón típico de **Policy Iteration**:\n",
    "- En las primeras iteraciones, la política cambia significativamente, lo que explica el alto número de evaluaciones internas.\n",
    "- Conforme se acerca a la política óptima, el número de iteraciones internas disminuye drásticamente (de 104 en la iteración 2 a solo 2 en la iteración 4).\n",
    "- La convergencia se logró en **4 iteraciones de mejora de política**, lo que confirma la eficiencia del método en entornos pequeños como GridWorld.\n",
    "\n",
    "La política final es estable y garantiza la obtención de la mayor recompensa posible desde cualquier estado no terminal.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Observaciones y dificultades\n",
    "- La implementación original estaba basada en `gym`, pero el entorno actual utilizaba `gymnasium`, lo que requirió modificar las importaciones para evitar conflictos.\n",
    "- Fue necesario verificar que el bucle de evaluación de política terminara correctamente, ya que umbrales muy bajos pueden aumentar innecesariamente el tiempo de cómputo.\n",
    "- Una dificultad inicial fue la interpretación de los valores de \\(V(s)\\) y su relación con las políticas intermedias, pero con la observación iterativa se confirmó que los valores altos coincidían con estados cercanos al objetivo.\n",
    "- El resultado final demuestra que el algoritmo es robusto y eficiente para problemas de decisión de Markov finitos.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusión:**  \n",
    "La implementación de **Policy Iteration** permitió al agente encontrar una política óptima en pocas iteraciones, con una convergencia clara tanto en la función de valor como en las acciones. La principal dificultad fue la compatibilidad de librerías, la cual se resolvió migrando completamente a `gymnasium`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
